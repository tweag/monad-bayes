

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>User Guide &mdash; monad-bayes  documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="monad-bayes  documentation" href="index.html"/>
        <link rel="next" title="Developer Guide" href="usage.html"/>
        <link rel="prev" title="Welcome to monad-bayes’s documentation!" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> monad-bayes
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#monad-bayes-vs-other-libraries">monad-bayes vs other libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
<li class="toctree-l2"><a class="reference internal" href="#specifying-distributions">Specifying distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#constructing-distributions-as-programs">Constructing distributions as programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hard-and-soft-conditioning">Hard and soft conditioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performing-inference">Performing inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#exact-inference">Exact inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#independent-forward-sampling">Independent forward sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#independent-weighted-sampling">Independent weighted sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#markov-chain-monte-carlo">Markov Chain Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sequential-monte-carlo-particle-filtering">Sequential Monte Carlo (Particle Filtering)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#resample-move-sequential-monte-carlo">Resample Move Sequential Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#particle-marginal-metropolis-hastings">Particle Marginal Metropolis Hastings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#interoperating-with-other-haskell-code">Interoperating with other Haskell code</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tips-on-writing-good-probabilistic-programs">Tips on writing good probabilistic programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#executables">Executables</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-docs">API docs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Developer Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">monad-bayes</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>User Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/probprog.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="user-guide">
<h1>User Guide<a class="headerlink" href="#user-guide" title="Permalink to this headline">¶</a></h1>
<p>Probabilistic programming is all about being able to write probabilistic models as programs. For instance, here is a Bayesian linear regression model:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">paramPriorRegression</span> <span class="ow">::</span> <span class="kt">MonadSample</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="p">(</span><span class="kt">Double</span><span class="p">,</span> <span class="kt">Double</span><span class="p">,</span> <span class="kt">Double</span><span class="p">)</span>
<span class="nf">paramPriorRegression</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">slope</span> <span class="ow">&lt;-</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">2</span>
    <span class="n">intercept</span> <span class="ow">&lt;-</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">2</span>
    <span class="n">noise</span> <span class="ow">&lt;-</span> <span class="n">gamma</span> <span class="mi">4</span> <span class="mi">4</span>
    <span class="n">return</span> <span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="nf">regression</span> <span class="ow">::</span> <span class="p">(</span><span class="kt">MonadInfer</span> <span class="n">m</span><span class="p">)</span> <span class="ow">=&gt;</span> <span class="p">[</span><span class="kt">Double</span><span class="p">]</span> <span class="ow">-&gt;</span> <span class="p">[</span><span class="kt">Double</span><span class="p">]</span> <span class="ow">-&gt;</span> <span class="n">m</span> <span class="p">(</span><span class="kt">Double</span><span class="p">,</span> <span class="kt">Double</span><span class="p">,</span> <span class="kt">Double</span><span class="p">)</span>
<span class="nf">regression</span> <span class="n">xs</span> <span class="n">ys</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">params</span><span class="o">@</span><span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span> <span class="ow">&lt;-</span> <span class="n">paramPriorRegression</span>
    <span class="n">forM</span> <span class="p">(</span><span class="n">zip</span> <span class="n">xs</span> <span class="n">ys</span><span class="p">)</span> <span class="nf">\</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">-&gt;</span> <span class="n">factor</span> <span class="o">$</span> <span class="n">normalPdf</span> <span class="p">(</span><span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="p">(</span><span class="n">sqrt</span> <span class="n">noise</span><span class="p">)</span> <span class="n">y</span>
    <span class="n">return</span> <span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">regression</span></code> takes observations of <code class="docutils literal notranslate"><span class="pre">xs</span></code> and <code class="docutils literal notranslate"><span class="pre">ys</span></code>, and using the prior expressed by <code class="docutils literal notranslate"><span class="pre">paramPriorRegression</span></code>, returns the posterior conditioned on the observations.</p>
<p>This is the <em>model</em>. To perform <em>inference</em> , suppose we have a data set of <code class="docutils literal notranslate"><span class="pre">xs</span></code> and <code class="docutils literal notranslate"><span class="pre">ys</span></code> like:</p>
<p><img alt="" src="_images/priorpred.png" /></p>
<p>We could then run the model as follows:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">mhRunsRegression</span> <span class="ow">=</span> <span class="n">sampleIO</span> <span class="o">$</span> <span class="n">prior</span> <span class="o">$</span> <span class="n">mh</span> <span class="mi">1000</span> <span class="o">$</span> <span class="n">regression</span> <span class="n">xs</span> <span class="n">ys</span>
</pre></div>
</div>
<p>This yields 1000 samples from an MCMC walk using an MH kernel. Plotting one gives:</p>
<p><img alt="" src="_images/regress.png" /></p>
<p>Monad-bayes provides a variety of MCMC and SMC methods, and methods arising from the composition of the two.</p>
<!-- `sprinkler` is a distribution over values for the Boolean `rain` variable given the likelihood and observation specified above. `enumerate` is a function which performs **inference**: it takes the abstract distribution `sprinkler` and calculates something concrete - in this case, the probability mass function.

`sprinkler` is specified as a program that has randomness (e.g. `bernoulli`) and scoring (e.g. `condition`). Hence the term *probabilistic programming*. The Grand Vision is that you write your statistical model as a probabilistic program and then choose or construct a method to perform inference in a statistically and computationally efficient way. -->
<section id="monad-bayes-vs-other-libraries">
<h2>monad-bayes vs other libraries<a class="headerlink" href="#monad-bayes-vs-other-libraries" title="Permalink to this headline">¶</a></h2>
<p>monad-bayes is a universal probabilistic programming language, in the sense that you can express any computable distribution. In this respect it differs from Stan, which focuses instead on handling inference on an important subset well.</p>
<p>There is a variety of universal probabilistic programming libraries and/or languages, which include WebPPL, Gen, Pyro and Edward.</p>
<p><strong>What other approaches have that monad-bayes lacks</strong>:</p>
<p>A lot of engineering work has been put into the above libraries and languages to make them practical for real-world problems. While monad-bayes’ core is very nice, it doesn’t come with a lot of the batteries you might want. (The author’s PhD thesis contains this relevant paragraph: “our library implements basic versions of advanced sampling algorithms. However, their successful application in practice requires incorporating established heuristics, such as: adaptive proposal distributions, controlling resampling with effective sample size, tuning rejuvenation kernels based on population in SMC2, and so on.”)</p>
<p><strong>What monad-bayes has that is unique</strong>:</p>
<p>Models are monadic and inference is modular. Complex inference algorithms like RMSMC or PMMH are built out of simple composable pieces, and so are expressable extraordinarily simply.</p>
<p>Probabilistic programs in monad-bayes are first class programs in Haskell. This allows all of Haskell’s expressive power to be brought to bear. You can write distributions over any datatype (lists, trees, functions, smart contracts, etc). You can use powerful libraries like Pipes, lens and Parsec. Everything is pure. You can make use of laziness. Everything is strongly typed. There’s no new special syntax or keywords.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Other probabilistic programming languages with fairly similar APIs include WebPPL and Gen. This cognitive-science oriented introduction to WebPPL is an excellent resource for learning about probabilistic programming: https://probmods.org/. The tutorials for Gen are also very good, particularly for learning about traces: https://github.com/probcomp/gen-quickstart/blob/master/tutorials/A%20Bottom-Up%20Introduction%20to%20Gen.ipynb.</p>
</section>
<section id="specifying-distributions">
<h2>Specifying distributions<a class="headerlink" href="#specifying-distributions" title="Permalink to this headline">¶</a></h2>
<p>A distribution in monad-bayes over a set $X$, is of type:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">X</span>
</pre></div>
</div>
<p>monad-bayes provides standard distributions, such as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">::</span> <span class="pre">MonadInfer</span> <span class="pre">m</span> <span class="pre">=&gt;</span> <span class="pre">m</span> <span class="pre">Double</span></code> : sample uniformly from $[0,1]$</p></li>
</ul>
<p>The full set is listed at https://hackage.haskell.org/package/monad-bayes-0.1.1.0/docs/Control-Monad-Bayes-Class.html</p>
<p>Note that these primitives already allows us to construct quite exotic distributions, like the uniform distribution over <code class="docutils literal notranslate"><span class="pre">(+)</span> <span class="pre">::</span> <span class="pre">Int</span> <span class="pre">-&gt;</span> <span class="pre">Int</span> <span class="pre">-&gt;</span> <span class="pre">Int</span></code> and <code class="docutils literal notranslate"><span class="pre">(-)</span> <span class="pre">::</span> <span class="pre">Int</span> <span class="pre">-&gt;</span> <span class="pre">Int</span> <span class="pre">-&gt;</span> <span class="pre">Int</span></code>:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">distributionOverFunctions</span> <span class="ow">=</span> <span class="n">uniformD</span> <span class="p">[(</span><span class="o">+</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="p">)]</span>
</pre></div>
</div>
<section id="constructing-distributions-as-programs">
<h3>Constructing distributions as programs<a class="headerlink" href="#constructing-distributions-as-programs" title="Permalink to this headline">¶</a></h3>
<p>monad-bayes also lets us construct new distributions out of these. <code class="docutils literal notranslate"><span class="pre">MonadInfer</span> <span class="pre">m</span></code> implies <code class="docutils literal notranslate"><span class="pre">Monad</span> <span class="pre">m</span></code> and in turn <code class="docutils literal notranslate"><span class="pre">Functor</span> <span class="pre">m</span></code>, so we can do the following:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">fmap</span> <span class="p">(</span><span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="n">random</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Bool</span>
</pre></div>
</div>
<p>This is the uniform distribution over $(0.5, 1]$.</p>
<p>As an important special case, if <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">::</span> <span class="pre">MonadInfer</span> <span class="pre">m</span> <span class="pre">=&gt;</span> <span class="pre">m</span> <span class="pre">(a,b)</span></code> is a joint distribution over two variables, then <code class="docutils literal notranslate"><span class="pre">fmap</span> <span class="pre">fst</span> <span class="pre">a</span> <span class="pre">::</span> <span class="pre">MonadInfer</span> <span class="pre">m</span> <span class="pre">=&gt;</span> <span class="pre">m</span> <span class="pre">a</span></code> <strong>marginalizes</strong> out the second variable. That is to say, <code class="docutils literal notranslate"><span class="pre">fmap</span> <span class="pre">fst</span> <span class="pre">a</span></code> is the distribution $p(a)$, where $p(a) = \int_b p(a,b)$</p>
<p>The above example use only the functor instance for <code class="docutils literal notranslate"><span class="pre">m</span></code>, but we also have the monad instance, as used in:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Double</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="n">bernoulli</span> <span class="mf">0.5</span> <span class="o">&gt;&gt;=</span> <span class="p">(</span><span class="nf">\</span><span class="n">x</span> <span class="ow">-&gt;</span> <span class="kr">if</span> <span class="n">x</span> <span class="kr">then</span> <span class="n">random</span> <span class="kr">else</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>It’s easiest to understand this distribution as a probabilistic program: it’s the distribution you get by first sampling from <code class="docutils literal notranslate"><span class="pre">bernoulli</span> <span class="pre">0.5</span></code>, then checking the result. If the result is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then sample from <code class="docutils literal notranslate"><span class="pre">random</span></code>, else from <code class="docutils literal notranslate"><span class="pre">normal</span> <span class="pre">0</span> <span class="pre">1</span></code>. As a distribution, this has a PDF:</p>
<p>$$ f(x) = 1[0\leq x \leq 1]*0.5  + \mathcal{N}(0,1)(x)*0.5  $$</p>
<!-- $$ \int\_{[0,1]} 1[x>0.5]* + (1[x\leq 0.5]*N(0,1)(x)) dx $$ -->
<p>Equivalently, we could write this in do-notation as:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Double</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">bool</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="kr">if</span> <span class="n">bool</span> <span class="kr">then</span> <span class="n">random</span> <span class="kr">else</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">1</span>
</pre></div>
</div>
<p><strong>A technical note</strong>: it is often tempting to read the line <code class="docutils literal notranslate"><span class="pre">bool</span> <span class="pre">&lt;-</span> <span class="pre">bernoulli</span> <span class="pre">0.5</span></code> as saying “take a sample from <code class="docutils literal notranslate"><span class="pre">bernoulli</span> <span class="pre">0.5</span></code>. But although we’ll see below that <code class="docutils literal notranslate"><span class="pre">example</span></code> can be interpreted as a sampler, there are many other interpretations, not least as a mathematical specification of a particular distribution.</p>
<p>That said, it is often useful to think of probabilistic programs as specifying distributions over <strong>program executation traces</strong>. For example, one trace of <code class="docutils literal notranslate"><span class="pre">example</span></code> as defined above is (informally): <code class="docutils literal notranslate"><span class="pre">{bernoulli</span> <span class="pre">0.5</span> <span class="pre">:</span> <span class="pre">True,</span> <span class="pre">random</span> <span class="pre">:</span> <span class="pre">0.7}</span></code>.</p>
</section>
<section id="hard-and-soft-conditioning">
<h3>Hard and soft conditioning<a class="headerlink" href="#hard-and-soft-conditioning" title="Permalink to this headline">¶</a></h3>
<p>monad-bayes provides a function <code class="docutils literal notranslate"><span class="pre">score</span> <span class="pre">::</span> <span class="pre">MonadInfer</span> <span class="pre">m</span> <span class="pre">=&gt;</span> <span class="pre">Log</span> <span class="pre">Double</span> <span class="pre">-&gt;</span> <span class="pre">m</span> <span class="pre">()</span></code>. (<strong>Note</strong>: <code class="docutils literal notranslate"><span class="pre">Log</span> <span class="pre">Double</span></code> is a wrapper for <code class="docutils literal notranslate"><span class="pre">Double</span></code> which stores doubles as their logarithm, and does multiplication by addition of logarithms.)</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Double</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">bool</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="n">number</span> <span class="ow">&lt;-</span> <span class="kr">if</span> <span class="n">bool</span> <span class="kr">then</span> <span class="n">random</span> <span class="kr">else</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">1</span>
  <span class="n">score</span> <span class="n">number</span> 
  <span class="n">return</span> <span class="n">bool</span>
</pre></div>
</div>
<p>It’s easiest to understand this in terms of the “program execution trace” perspective described above. What the score statement does is to multiple every trace by the value of <code class="docutils literal notranslate"><span class="pre">number</span></code> in that particular trace.</p>
<p><code class="docutils literal notranslate"><span class="pre">condition</span></code> in then defined as follows:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">condition</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="kt">Bool</span> <span class="ow">-&gt;</span> <span class="n">m</span> <span class="nb">()</span>
<span class="nf">condition</span> <span class="n">b</span> <span class="ow">=</span> <span class="n">score</span> <span class="o">$</span> <span class="kr">if</span> <span class="n">b</span> <span class="kr">then</span> <span class="mi">1</span> <span class="kr">else</span> <span class="mi">0</span>
</pre></div>
</div>
<p>So <code class="docutils literal notranslate"><span class="pre">condition</span> <span class="pre">b</span></code> throws away every trace in which <code class="docutils literal notranslate"><span class="pre">b</span></code> is False, and keeps all traces in which <code class="docutils literal notranslate"><span class="pre">b</span></code> is True. For example:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Int</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">n</span> <span class="ow">&lt;-</span> <span class="n">poisson</span> <span class="mf">0.5</span>
  <span class="n">condition</span> <span class="p">(</span><span class="n">n</span><span class="o">%</span><span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">return</span> <span class="n">n</span>
</pre></div>
</div>
<p>This describes a poisson distribution in which all even values of the random variable are marginalized out.</p>
<!-- Another use case is Bayesian inference as in:

<!-- The most intuitive way to understand `score` is to think of a probabilistic program as making a series of random choices which trace out a possible execution of the program. At any point in this series, we can interject a `score x` statement, where the value of `x` depends on the previous choices. This statement multiplies the weight of this "trace" by the score. -->
<!-- ```haskell
bayesianExample :: (Eq a, MonadInfer m) => m a -> (a -> m b) -> (b -> m a)
bayesianExample prior likelihood b = do
    a <- prior
    b' <- likelihood a
    condition (b==b')
    return a
```

Note that operationally speaking, this approach is only going to work well for discrete distributions, since `b==b'` is going to be zero-measure in the continuous case. But in the discrete case, we could for example do: -->
<!-- ```haskell
example :: MonadInfer 
example =  bayesianExample (bernoulli 0.5) (\x -> if x then bernoulli 0.8 else bernoulli 0.9) True 
``` 
-->
<!-- ```haskell
example :: MonadInfer m => m Bool
example = do 
  x <- normal 0 1
  y <- normal 0 2
  z <- normal 0 3
  return (x > y)
```

Note that in this example, commenting out the line `z <- normal 0 3` would not change the distribution at all. **But**, there is no guarantee in theory that the inference method you use knows this. More generally,  -->
<!-- **Not all ways of expressing denotationally equivalent distributions are equally useful in practice** -->
</section>
</section>
<section id="performing-inference">
<h2>Performing inference<a class="headerlink" href="#performing-inference" title="Permalink to this headline">¶</a></h2>
<p>To quote <a class="reference external" href="https://webppl.readthedocs.io/en/master/inference/">this page</a>, “marginal inference (or just inference) is the process of reifying the distribution on return values implicitly represented by a stochastic computation.”. That is, a probabilistic program (stochastic computation) is an abstract object and inference transforms it into something concrete, like a histogram, a list of samples, or parameters of a known distribution.</p>
<p>All inference methods in monad-bayes work with all distributions. The only exception is that exact inference only works with discrete distributions and will throw a runtime error on continuous distributions.</p>
<p><strong>The challenge of inference</strong> is that most distributions that are of interest are not as simple as <code class="docutils literal notranslate"><span class="pre">sprinkler</span></code>. They could have continuous random variables, a huge number of them, or even a number of them that is itself random. They could involve a series of observations, interspersed with other sources of randomness.</p>
<p>Designing a language in which you can specify arbitrarily complex (computable) distributions as probabilistic programs turns out to be a largely solved problem. The tools given about are sufficient for that.</p>
<p>The hard part is designing a language where you can specify how you want to do inference, because sophisticated, often approximate, inference methods are almost always necessary for the models involved in solving real world problems.</p>
<p>Two of the large classes of inference methods are <strong>sampling based methods</strong> and <strong>gradient based methods</strong>. The latter only apply to continuous probability distributions, and are not the focus of monad-bayes.</p>
<!-- For the purposes of this section, let `dist :: MonadInfer m => m a` be the distribution you want to perform inference on.  -->
<section id="exact-inference">
<h3>Exact inference<a class="headerlink" href="#exact-inference" title="Permalink to this headline">¶</a></h3>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">enumerate</span> <span class="ow">::</span> <span class="kt">Ord</span> <span class="n">a</span> <span class="ow">=&gt;</span> <span class="kt">Enumerator</span> <span class="n">a</span> <span class="ow">-&gt;</span> <span class="p">[(</span><span class="n">a</span><span class="p">,</span> <span class="kt">Double</span><span class="p">)]</span>
</pre></div>
</div>
<p>So <code class="docutils literal notranslate"><span class="pre">enumerate</span> <span class="pre">(bernoulli</span> <span class="pre">0.7)</span></code> gives you</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="kc">False</span><span class="p">,</span><span class="mf">0.3</span><span class="p">),(</span><span class="kc">True</span><span class="p">,</span><span class="mf">0.7</span><span class="p">)]</span>
</pre></div>
</div>
<p><strong>Note: enumerate only works on finite discrete distributions</strong></p>
<p>It will run forever on infinite distributions like <code class="docutils literal notranslate"><span class="pre">enumerate</span> <span class="pre">(poisson</span> <span class="pre">0.7)</span></code> and will throw the following <strong>runtime</strong> error on continuous distributions as in <code class="docutils literal notranslate"><span class="pre">enumerate</span> <span class="pre">(normal</span> <span class="pre">0</span> <span class="pre">1)</span></code>:</p>
<p><em>“Exception: Infinitely supported random variables not supported in Enumerator”</em></p>
<p><strong>However</strong>, it’s totally fine to have the elements of the support to be infinite but discrete, as in:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">fmap</span> <span class="p">(</span><span class="nf">\</span><span class="p">(</span><span class="n">ls</span><span class="p">,</span><span class="n">p</span><span class="p">)</span> <span class="ow">-&gt;</span> <span class="p">(</span><span class="n">take</span> <span class="mi">4</span> <span class="n">ls</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span> <span class="o">$</span> <span class="n">enumerate</span> <span class="o">$</span> <span class="n">uniformD</span> <span class="p">[[</span><span class="mi">1</span><span class="o">..</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="o">..</span><span class="p">]]</span>
</pre></div>
</div>
<p>which gives</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span><span class="mf">0.5</span><span class="p">),([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="mf">0.5</span><span class="p">)]</span>
</pre></div>
</div>
</section>
<section id="independent-forward-sampling">
<h3>Independent forward sampling<a class="headerlink" href="#independent-forward-sampling" title="Permalink to this headline">¶</a></h3>
<p>For any probabilistic program <code class="docutils literal notranslate"><span class="pre">p</span></code> without any <code class="docutils literal notranslate"><span class="pre">condition</span></code> or <code class="docutils literal notranslate"><span class="pre">factor</span></code> statements, we may do <code class="docutils literal notranslate"><span class="pre">sampleIO</span> <span class="pre">p</span></code> or <code class="docutils literal notranslate"><span class="pre">sampleSTfixed</span> <span class="pre">p</span></code> (to run with a fixed seed) to obtain a sample in an ancestral fashion. For example, consider:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="kr">if</span> <span class="n">x</span> <span class="kr">then</span> <span class="n">normal</span> <span class="mi">0</span> <span class="mi">1</span> <span class="kr">else</span> <span class="n">normal</span> <span class="mi">1</span> <span class="mi">2</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">sampleIO</span> <span class="pre">example</span></code> will produce a sample from a Bernoulli distribution with $p=0.5$, and if it is $True$, return a sample from a standard normal, else from a normal with mean 1 and std 2. ‘</p>
<p><code class="docutils literal notranslate"><span class="pre">(replicateM</span> <span class="pre">n</span> <span class="pre">.</span> <span class="pre">sampleIO)</span> <span class="pre">example</span></code> will produce a list of <code class="docutils literal notranslate"><span class="pre">n</span></code> independent samples. However, it is recommended to instead do <code class="docutils literal notranslate"><span class="pre">(sampleIO</span> <span class="pre">.</span> <span class="pre">replicateM</span> <span class="pre">n)</span> <span class="pre">example</span></code>, which will create a new model (<code class="docutils literal notranslate"><span class="pre">replicateM</span> <span class="pre">n</span> <span class="pre">example</span></code>) consisting of <code class="docutils literal notranslate"><span class="pre">n</span></code> independent draws from <code class="docutils literal notranslate"><span class="pre">example</span></code>.</p>
<p>Because <code class="docutils literal notranslate"><span class="pre">sampleIO</span> <span class="pre">example</span></code> is totally pure, it is parallelizable.</p>
</section>
<section id="independent-weighted-sampling">
<h3>Independent weighted sampling<a class="headerlink" href="#independent-weighted-sampling" title="Permalink to this headline">¶</a></h3>
<p>To perform weighted sampling, use:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">runWeighted</span><span class="p">)</span> <span class="ow">::</span> <span class="kt">Weighted</span> <span class="kt">SamplerIO</span> <span class="n">a</span> <span class="ow">-&gt;</span> <span class="kt">IO</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="kt">Log</span> <span class="kt">Double</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Weighted</span> <span class="pre">SamplerIO</span></code> is an instance of <code class="docutils literal notranslate"><span class="pre">MonadInfer</span></code>, so we can apply this to any distribution. For example, suppose we have the distribution:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Bool</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="n">condition</span> <span class="n">x</span>
  <span class="n">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Then:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">run</span> <span class="ow">::</span> <span class="kt">IO</span> <span class="p">(</span><span class="kt">Bool</span><span class="p">,</span> <span class="kt">Log</span> <span class="kt">Double</span><span class="p">)</span>
<span class="nf">run</span> <span class="ow">=</span> <span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">runWeighted</span><span class="p">)</span> <span class="n">example</span>
</pre></div>
</div>
<p>is an IO operation which when run, will display either <code class="docutils literal notranslate"><span class="pre">(False,</span> <span class="pre">0.0)</span></code> or <code class="docutils literal notranslate"><span class="pre">(True,</span> <span class="pre">1.0)</span></code></p>
</section>
<section id="markov-chain-monte-carlo">
<h3>Markov Chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Permalink to this headline">¶</a></h3>
<p>There are several versions of metropolis hastings MCMC defined in monad-bayes. The standard version is found in Control.Monad.Bayes.Traced. You can use it as follows:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">prior</span> <span class="o">.</span> <span class="n">mh</span> <span class="n">numSteps</span><span class="p">)</span> <span class="ow">::</span> <span class="kt">Traced</span> <span class="p">(</span><span class="kt">Weighted</span> <span class="kt">SamplerIO</span><span class="p">)</span> <span class="n">a</span> <span class="ow">-&gt;</span> <span class="kt">IO</span> <span class="p">[</span><span class="n">a</span><span class="p">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Traced</span> <span class="pre">(Weighted</span> <span class="pre">SamplerIO)</span></code> is an instance of <code class="docutils literal notranslate"><span class="pre">MonadInfer</span></code>, so we can apply this to any distribution. For instance:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Bool</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="n">condition</span> <span class="n">x</span>
  <span class="n">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Then</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">run</span> <span class="ow">::</span> <span class="kt">IO</span> <span class="p">[</span><span class="kt">Bool</span><span class="p">]</span>
<span class="nf">run</span> <span class="ow">=</span> <span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">prior</span> <span class="o">.</span> <span class="n">mh</span> <span class="mi">10</span><span class="p">)</span> <span class="n">example</span>
</pre></div>
</div>
<p>produces 10 unbiased samples from the posterior, by using single-site trace MCMC with the Metropolis-Hastings (MH) method. This means that the random walk is over execution traces of the probabilistic program, and the proposal distribution modifies a single random variable as a time, and then uses MH for the accept-reject criterion. For example, from the above you’d get:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">]</span>
</pre></div>
</div>
<p>The end of the chain is the head of the list, so you can drop samples from the end of the list for burn-in.</p>
</section>
<section id="sequential-monte-carlo-particle-filtering">
<h3>Sequential Monte Carlo (Particle Filtering)<a class="headerlink" href="#sequential-monte-carlo-particle-filtering" title="Permalink to this headline">¶</a></h3>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">sampleIO</span><span class="o">.</span> <span class="n">runPopulation</span> <span class="o">.</span> <span class="n">smcSystematic</span> <span class="n">numSteps</span> <span class="n">numParticles</span><span class="p">)</span> 
  <span class="ow">::</span> <span class="kt">Sequential</span> <span class="p">(</span><span class="kt">Population</span> <span class="kt">SamplerIO</span><span class="p">)</span> <span class="n">a</span> <span class="ow">-&gt;</span> <span class="kt">IO</span> <span class="p">[(</span><span class="n">a</span><span class="p">,</span> <span class="kt">Numeric</span><span class="o">.</span><span class="kt">Log</span><span class="o">.</span><span class="kt">Log</span> <span class="kt">Double</span><span class="p">)]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Sequential</span> <span class="pre">(Population</span> <span class="pre">SamplerIO)</span></code> is an instance of <code class="docutils literal notranslate"><span class="pre">MonadInfer</span></code>, so we can apply this inference method to any distribution. For instance, to use our now familiar <code class="docutils literal notranslate"><span class="pre">example</span></code>:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">::</span> <span class="kt">MonadInfer</span> <span class="n">m</span> <span class="ow">=&gt;</span> <span class="n">m</span> <span class="kt">Bool</span>
<span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="n">condition</span> <span class="n">x</span>
  <span class="n">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Then</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">run</span> <span class="ow">::</span> <span class="kt">IO</span> <span class="p">[(</span><span class="kt">Bool</span><span class="p">,</span> <span class="kt">Log</span> <span class="kt">Double</span><span class="p">)]</span>
<span class="nf">run</span> <span class="ow">=</span> <span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">runPopulation</span><span class="o">.</span> <span class="n">smcSystematic</span> <span class="mi">4</span> <span class="mi">4</span><span class="p">)</span> <span class="n">example</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="kc">True</span><span class="p">,</span><span class="mf">6.25e-2</span><span class="p">),(</span><span class="kc">True</span><span class="p">,</span><span class="mf">6.25e-2</span><span class="p">),(</span><span class="kc">True</span><span class="p">,</span><span class="mf">6.25e-2</span><span class="p">),(</span><span class="kc">True</span><span class="p">,</span><span class="mf">6.25e-2</span><span class="p">)]</span>
</pre></div>
</div>
<p>Each of these is a particle with a weight. In this simple case, there are all identical - obviously in general they won’t be.</p>
<p><code class="docutils literal notranslate"><span class="pre">numSteps</span></code> is the number of steps that the <code class="docutils literal notranslate"><span class="pre">SMC</span></code> algorithm takes, i.e. how many times it resamples. This should generally be the number of factor statements in the program. <code class="docutils literal notranslate"><span class="pre">numParticles</span></code> is the size of the population. Larger is better but slower.</p>
</section>
<section id="resample-move-sequential-monte-carlo">
<h3>Resample Move Sequential Monte Carlo<a class="headerlink" href="#resample-move-sequential-monte-carlo" title="Permalink to this headline">¶</a></h3>
<p>This is a fancier variant of SMC, which has the particles take an MCMC walk through the solution space in order to spread out. This can avoid a common failure mode of SMC, where the population concentrates its weight too heavily on one mode.</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">rmsmcBasic</span> <span class="ow">::</span>
  <span class="kt">MonadSample</span> <span class="n">m</span> <span class="ow">=&gt;</span>
  <span class="c1">-- | number of timesteps</span>
  <span class="kt">Int</span> <span class="ow">-&gt;</span>
  <span class="c1">-- | number of particles</span>
  <span class="kt">Int</span> <span class="ow">-&gt;</span>
  <span class="c1">-- | number of Metropolis-Hastings transitions after each resampling</span>
  <span class="kt">Int</span> <span class="ow">-&gt;</span>
  <span class="c1">-- | model</span>
  <span class="kt">Sequential</span> <span class="p">(</span><span class="kt">Traced</span> <span class="p">(</span><span class="kt">Population</span> <span class="n">m</span><span class="p">))</span> <span class="n">a</span> <span class="ow">-&gt;</span>
  <span class="kt">Population</span> <span class="n">m</span> <span class="n">a</span>
</pre></div>
</div>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">run</span> <span class="ow">::</span> <span class="kt">IO</span> <span class="p">[(</span><span class="kt">Bool</span><span class="p">,</span> <span class="kt">Log</span> <span class="kt">Double</span><span class="p">)]</span>
<span class="nf">run</span> <span class="ow">=</span> <span class="p">(</span><span class="n">sampleIO</span> <span class="o">.</span> <span class="n">runPopulation</span><span class="o">.</span> <span class="n">smcSystematic</span> <span class="mi">4</span> <span class="mi">4</span> <span class="mi">4</span><span class="p">)</span> <span class="n">example</span>
</pre></div>
</div>
<p>What this returns is a population of samples, just like plain <code class="docutils literal notranslate"><span class="pre">SMC</span></code>. The third argument to <code class="docutils literal notranslate"><span class="pre">rmsmcBasic</span></code> is the number of MCMC steps taken after each resampling. More is better, but slower.</p>
<!-- todo -->
</section>
<section id="particle-marginal-metropolis-hastings">
<h3>Particle Marginal Metropolis Hastings<a class="headerlink" href="#particle-marginal-metropolis-hastings" title="Permalink to this headline">¶</a></h3>
<p>This inference method takes a prior and a model separately, so it only applies to a (large) subset of probabilistic programs.</p>
<!-- Run it like this: -->
<!-- todo -->
<!-- Here I use "inference" to mean the process of getting from the distribution in the abstract the something concrete, like samples from it,  an expectation over it, parameters of it, or in the above case of `enumerate`, the mass of each element of the support. -->
<!-- You then want to be able to convert this abstract specification of a distribution or model into something tangible, and in the case of this simple discrete distribution, we can do so by brute force. That's what `enumerate` does. -->
<!-- It feels natural that a pure, functional, strongly typed language like Haskell should have a good story for Bayesian probability, inference, and probabilistic programming.  -->
<!-- denotation of probabilistic programs, which we then are free to interpret in myriad ways: as weighted lists, samplers, or a variety of more sophisticated programs for performing inference -->
<!-- In particular, these interpretations can be combined, and by so doing, you can built up really rather complex inference algorithms while being sure that your method is sound. And also intelligible.  -->
<!-- *The interpretation of your model is the program which performs inference on it* -->
<!-- # Example Gallery -->
<!-- todo: link to monad-bayes examples, with graphs and how to run -->
<!-- `sprinkler` above is a great example of the two new things you can do in a probabilistic program that you can't do in other programs: you can draw from distributions, and you can *condition* on observations. For example:

```haskell
example = do
    ind <- fmap not (bernoulli 0.9)
    val <- if ind then gaussian 0 1 else poisson 0.5
    condition (val > 1)
    return ind
```

This example is contrived, in order to show a few things. First, `m` in distributions like `bernoulli 0.9 :: MonadSample m => m Bool` (the distribution `{True : 0.9, False: 0.1}`) are functors, so we can fmap over them, e.g. with `not` to get the distribution `{True : 0.1, False: 0.9}`. Second, distributions are monads, so we can draw from them and use the results as the parameters of other distributions. Third, we have a `condition` function, which throws out all values of `ind` which would result in `val <= 1`.

The fact that distributions are a monad is the essence of probabilistic programming. It allows you to express everything from simple models (Bayesian linear regression) to complex ones (hierarchical latent Dirichlet models) in a shared language. See the `models` folder (TODO LINK) for examples.

```haskell
betaBernoulli :: MonadSample m => Int -> m [Bool]
betaBernoulli n = do
  weight <- uniform 0 1
  let toss = bernoulli weight
  replicateM n toss
```

 -->
</section>
</section>
<section id="interoperating-with-other-haskell-code">
<h2>Interoperating with other Haskell code<a class="headerlink" href="#interoperating-with-other-haskell-code" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic programs in monad-bayes are Haskell programs. This contrasts to many probabilistic programming languages, which are deeply embedded and cannot smoothly interact with their host language.</p>
<p>For example, we can use ordinary monadic combinators, as in:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">=</span> <span class="kr">do</span>
  <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
  <span class="n">when</span> <span class="n">x</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.8</span><span class="p">)</span>
  <span class="n">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">example</span> <span class="ow">=</span> <span class="n">whileM</span> <span class="p">(</span><span class="n">bernoulli</span> <span class="mf">0.99</span><span class="p">)</span> <span class="p">(</span><span class="n">normal</span> <span class="mi">0</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<!-- We can use libraries like Pipes, to specify lazy distributions as in models/Pipes.hs

We can write probabilistic optics to update or view latent variables, as in models/Lens.hs.

We can define models like PCFGs using recursion schemes, as in models/PCFG.hs.

We can write probabilistic parsers, as in models/Parser.hs.

We can use monad transformers on top of our probability monad `m`, as in models/Failure.hs. -->
<!-- And, because we're programming directly in Haskell, rather than a domain specific language (like Church, Gen, WebPPL and most other probabilistic programming languages), we can interoperate with any other Haskell concepts. Two examples: -->
</section>
<section id="tips-on-writing-good-probabilistic-programs">
<h2>Tips on writing good probabilistic programs<a class="headerlink" href="#tips-on-writing-good-probabilistic-programs" title="Permalink to this headline">¶</a></h2>
<p>There are many ways to specify the same distribution, and some will lend themselves more readily to efficient inference than others. For instance,</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">mixture1</span> <span class="n">point</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">cluster</span> <span class="ow">&lt;-</span> <span class="n">uniformD</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">prediction</span> <span class="ow">&lt;-</span> <span class="n">normal</span> <span class="n">cluster</span> <span class="mi">1</span>
    <span class="n">condition</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">==</span> <span class="n">point</span> <span class="p">)</span>
    <span class="n">return</span> <span class="n">cluster</span>
</pre></div>
</div>
<p>is a piece of code to infer whether an observed point was generated from a Gaussian of mean $1$ or $5$. That is, <code class="docutils literal notranslate"><span class="pre">mixture1</span></code> is a conditional Bernoulli distribution over the mean given an observation. You’re not going to be able to do much with <code class="docutils literal notranslate"><span class="pre">mixture1</span></code> though. Exact inference is impossible because of the sample from the normal, and as for sampling, there is zero probability of sampling the normal to exactly match the observed point, which is what the <code class="docutils literal notranslate"><span class="pre">condition</span></code> requires.</p>
<p>However, the same conditional distribution is represented by</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">mixture2</span> <span class="n">point</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">cluster</span> <span class="ow">&lt;-</span> <span class="n">uniformD</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">factor</span> <span class="p">(</span><span class="n">normalPdf</span> <span class="n">cluster</span> <span class="mi">1</span> <span class="n">point</span><span class="p">)</span>
    <span class="n">return</span> <span class="n">cluster</span>
</pre></div>
</div>
<p>This version, while <em>denotational identical</em> (i.e. representing the same mathematical object), is perfectly amenable to exact inference:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">enumerate</span> <span class="o">$</span> <span class="n">mixture2</span> <span class="mi">2</span>
</pre></div>
</div>
<p>yields</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.98</span><span class="o">...</span><span class="p">),</span> <span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.017</span><span class="o">...</span><span class="p">)]</span>
</pre></div>
</div>
<p>as well as sampling methods.</p>
<p>The local lesson here is that you shouldn’t <code class="docutils literal notranslate"><span class="pre">condition</span></code> on samples from a continuous distribution and expect a sampling based inference method to work. But the more general lesson is that you aren’t exempted from thinking about inference when specifying your model. Alas.</p>
<p>As a second example of this general point, consider:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">allAtOnce</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
    <span class="n">y</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
    <span class="n">condition</span> <span class="p">(</span><span class="n">x</span> <span class="o">&amp;&amp;</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&amp;&amp;</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>and</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">incremental</span> <span class="ow">=</span> <span class="kr">do</span>
    <span class="n">x</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
    <span class="n">condition</span> <span class="n">x</span>
    <span class="n">y</span> <span class="ow">&lt;-</span> <span class="n">bernoulli</span> <span class="mf">0.5</span>
    <span class="n">condition</span> <span class="n">y</span>
    <span class="n">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&amp;&amp;</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Like in the previous example, <code class="docutils literal notranslate"><span class="pre">allAtOnce</span></code> and <code class="docutils literal notranslate"><span class="pre">incremental</span></code> denote the same distribution, namely <code class="docutils literal notranslate"><span class="pre">[(True,</span> <span class="pre">1.0),</span> <span class="pre">(False,</span> <span class="pre">0.0)]</span></code>. However, any inference algorithm for <code class="docutils literal notranslate"><span class="pre">allAtOnce</span></code> will have to explore all 4 possible variable assignments (<code class="docutils literal notranslate"><span class="pre">[(True,True),</span> <span class="pre">(True,</span> <span class="pre">False),</span> <span class="pre">(False,</span> <span class="pre">True),</span> <span class="pre">(False,</span> <span class="pre">False)]</span></code>). Meanwhile, <code class="docutils literal notranslate"><span class="pre">incremental</span></code> opens the possibility for inference algorithms to first determine the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> and then of <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>In this example, the performance difference is negligible, but it’s easy to extend this to models where it’s the difference between something tractable and something intractable.</p>
<!-- todo: similar lesson about incremental factors: compare two models -->
</section>
<section id="executables">
<h2>Executables<a class="headerlink" href="#executables" title="Permalink to this headline">¶</a></h2>
<p>monad-bayes comes with an executable called <code class="docutils literal notranslate"><span class="pre">example</span></code>. It’s not particularly useful, except as a reference to see a compiled program which generates data, performs inference and reports the results. Once you’ve done <code class="docutils literal notranslate"><span class="pre">stack</span> <span class="pre">build</span></code>, run this with e.g.:</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">stack</span> <span class="n">exec</span> <span class="n">example</span> <span class="c1">-- -m LDA4 -a MH</span>
</pre></div>
</div>
<p>where the options for <code class="docutils literal notranslate"><span class="pre">-m</span></code> (model) are “LDA” (latent dirichlet), “LR” (logistic regression) and “HMM” (hidden Markov model) and for <code class="docutils literal notranslate"><span class="pre">-a</span></code> (algorithm) are “MH” (Metropolis Hastings), “SMC” (sequential Monte Carlo), and “RMSMC” (resample-move sequential Monte Carlo). The number is the number of steps to take.</p>
</section>
<section id="api-docs">
<h2>API docs<a class="headerlink" href="#api-docs" title="Permalink to this headline">¶</a></h2>
<p>For API docs, see <a class="reference external" href="https://hackage.haskell.org/package/monad-bayes">hackage</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="usage.html" class="btn btn-neutral float-right" title="Developer Guide" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Welcome to monad-bayes’s documentation!" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 1980, Adam Scibior.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> and ❤️  using a custom <a href="https://github.com/LinxiFan/Sphinx-theme">theme</a> based on <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/documentation_options.js"></script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>